train:
  batch_size: 32
  adam:
    lr: 2.2e-4 # baseline 학습/warm start 시에는 1e-3
    weight_decay: 1e-6
  decay:
    rate: 0.1
    start: 5000
    end: 20000
  num_workers: 16
  gpus: 2 #ddp
  loss_rate:
    dur: 2.0

data:
  lang: 'eng'
  text_cleaners: ['english_cleaners'] # korean_cleaners, english_cleaners, chinese_cleaners
  speakers: ['LJSpeech']
  train_dir: 'preprocessed_data/LJSpeech'
  train_meta: 'train.txt'  # relative path of metadata file from train_dir
  val_dir: 'preprocessed_data/LJSpeech'
  val_meta: 'val.txt'  # relative path of metadata file from val_dir'

audio:
  sampling_rate: 22050
  nfft: 1024
  hop: 256
  ratio: 2 #upscale_ratio
  length: 32768 #32*1024 ~ 1sec

chn:
  encoder: 512
  speaker: 64
  # decoder
  prenet: 256
  postnet: 512
  attention_rnn: 1024
  attention: 128
  decoder_rnn: 1024
  ### NON-ATTENTIVE ###
  dur_lstm: 512
  range_lstm: 512
ker:
  encoder: 5
  postnet: 5
depth:
  encoder: 3
  prenet: 2
  postnet: 5

window:
  scale: 300
  length: 64

arch:
  encode_channel: 576 ##need to check
  scale_factors: [5,5,3,2,2]
  upsample:
    preconv_channel: 768
    out_channels: [512, 512, 256, 128, 128]
    dilations: [[1,2,1,2],[1,2,1,2],[1,2,4,8],[1,2,4,8],[1,2,4,8]]
  downsample:
    preconv_channel: 32
    out_channels: [128, 128, 256, 512]
    dilations: [[1,2,4],[1,2,4],[1,2,4],[1,2,4]]
  pos_emb_dim: 512 

ddpm:
  max_step: 1000
  noise_schedule: "torch.linspace(1e-6, 0.006, hparams.ddpm.max_step)"
  pos_emb_scale: 50000
  pos_emb_channels: 128 
  infer_step: 8
  infer_schedule: "torch.tensor([1e-6,2e-6,1e-5,1e-4,1e-3,1e-2,1e-1,9e-1])"

log:
  name: 'wavegrad2'
  checkpoint_dir: 'checkpoint'
  tensorboard_dir: 'tensorboard'
  test_result_dir: 'test_sample/result'
